{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8df230",
   "metadata": {},
   "source": [
    "# Tracing Correlation between Online Sentiment and Big Tech Stock Prices:      A Case Study of Meta's Layoff News"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba11b8",
   "metadata": {},
   "source": [
    "## Background and Motivations for Research\n",
    "\n",
    "In stark contrast to its mid-pandemic boom, the tech industry is facing a number of challenges including rising inflation and a potential recession that constrains company spending, and increased scrutiny from regulators around the world that makes it more difficult to operate and grow.\n",
    "\n",
    "Furthermore, chain bankruptcies in the banking industry including Silicon Valley Bank’s collapse make it difficult for tech companies to access capital for essential company operations(__[Daily Mail, 2023](https://www.dailymail.co.uk/news/article-11845803/SVB-collapse-major-impact-tech-industry.html)__). All of these factors combined have led leading tech companies like Meta, Google, and Amazon to announce mass layoffs, raising concerns about the health of the tech industry and the future of the tech workforce.\n",
    "\n",
    "Meta was the first among Big Tech to announce mass layoffs due to rising costs, declining ad revenue, and increased competition from companies like TikTok and Snapchat (__[CNN, 2023](https://www.cnn.com/2023/04/19/tech/meta-tech-team-layoffs-begin/index.html#:~:text=Meta%20has%20said%20the%20layoffs,building%20the%20so%2Dcalled%20metaverse)__). It announced two rounds of layoffs in the past year. The first round, announced on November 9th 2022 (__[Meta, 2022](https://about.fb.com/news/2022/11/mark-zuckerberg-layoff-message-to-employees/)__), affected 11,000 employees. The second round, announced on March 14th 2023 (__[Meta, 2023](https://about.fb.com/news/2023/03/mark-zuckerberg-meta-year-of-efficiency/)__), affected 10,000 employees.\n",
    "\n",
    "In light of such macroeconomic events, I thought it might be an interesting time to see if online sentiment of a company’s events would have an impact on its stock performance. How would investors perceive an event like mass layoffs? Do their perceptions translate into real returns in the stock market?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d47b6a",
   "metadata": {},
   "source": [
    "## Research Questions and Hypotheses\n",
    "In this project, I will investigate the impact of online sentiment on the share prices of big tech companies. I use Meta’s layoffs as a case study as it was the first Big Tech company to announce mass layoffs and there was the most buzz around its decision online. My research questions are:\n",
    "\n",
    "<ul>\n",
    "<li>Does online sentiment regarding Meta's layoffs have an impact on (or at least a correlation with) Meta's share prices?</li>\n",
    "<li>How do investors and the public perceive layoffs? What about a second round of layoffs?</li>\n",
    "<li>Which publishers have more predictive/explanatory power?</li>\n",
    "</ul>\n",
    "\n",
    "My hypotheses are the following:\n",
    "<ul>\n",
    "    <li>There is a linear relationship between online sentiment expressed on Meta's layoffs news and its share prices. Negative online sentiments are expected to have a negative impact on share prices. </li>\n",
    "    <li>The first round of layoffs might alarm investors (negative online sentiment) and drive down stock prices, but a second round of layoffs might signal business frugality (positive online sentiment) and lead to higher stock prices, especially as other tech companies follow through with the trend of laying off workers. </li>\n",
    "    <li> Financial advisory websites like The Motley Fool and Seeking Alpha have more predictive power on the stock market than financial news outlets like The Wall Street Journal or Financial Times. That is, the trends in their online sentiments will be more closely aligned with the trends in the stock market. I hypothesize so because these websites often help retail investors buy suggesting whether to buy or sell a stock. </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79f3f8",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "I use two sources of data. My historical stock price data comes from [Yahoo Finance](https://finance.yahoo.com/quote/META/history/), which describes the opening and closing stock prices of Meta for the past year. My sentiment data comes from [Aylien](https://aylien.com/product/news-api), a news scraping website similar to News API. Because I was using the free version, I was only able to scrape 100 articles per search. However, Aylien had an advantage over News API in that it allowed me to scrape articles published in 6 months ago (News API only lets you scrape articles published up to a month ago). Using Aylien’s API, I was able to access articles published by financial news outlets and financial advisory websites. Here are the publishers I specified in my API parameters:\n",
    "\n",
    "<ul>\n",
    "<li>Financial news outlets: Wall Street Journal, Reuters, Financial Times, Forbes, The Economist, Bloomberg, Yahoo! Finance, CNBC, CNN, Business Insider, and TechCrunch</li>\n",
    "<li>Financial advisory websites: Market Watch, The Motley Fool, Seeking Alpha, Zacks Investment Research, Money Morning, Stock Rover, Morningstar</li>\n",
    "</ul>\n",
    "\n",
    "I filtered the articles according to date published and keywords in the title and body. For my article dates, I picked a range of 10 days (6 days before and 4 days after the layoff announcements) to see the day trends before and after the announcement. I create 4 data frames to complete my analysis. They are data frames containing data from:\n",
    "\n",
    "<ul>\n",
    "    <li>Articles on first round layoffs in November, 2022 from financial news outlets </li>\n",
    "    <li>Articles on first round layoffs in November, 2022 from financial advisory websites </li>\n",
    "    <li>Articles on second round layoffs in March, 2023 from financial news outlets </li>\n",
    "    <li>Articles on second round layoffs in March, 2023 from financial advisory websites </li>\n",
    "</ul>\n",
    "\n",
    "Here are my code blocks for calling the Aylien API, importing the data, and storing them in data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf734fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2af011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the csv file containing stock price data\n",
    "df_stock_march = pd.read_csv('Meta_stock_price.csv')\n",
    "df_stock_nov = pd.read_csv('Meta_stock_price.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4b316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_stock_march['Date'] = pd.to_datetime(df_stock_march['Date'])\n",
    "df_stock_nov['Date'] = pd.to_datetime(df_stock_nov['Date'])\n",
    "\n",
    "# Filter the data for March and November data\n",
    "df_stock_march = df_stock_march[(df_stock_march['Date'] >= '2023-03-08') & (df_stock_march['Date'] <= '2023-03-18')]\n",
    "df_stock_nov = df_stock_nov[(df_stock_nov['Date'] >= '2022-11-03') & (df_stock_nov['Date'] <= '2022-11-13')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af41e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U aylien-news-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66511c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from the Aylien “Python SDK” tutorial: https://docs.aylien.com/newsapi/sdks/#python-sdk\n",
    "\n",
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint as pp\n",
    "\n",
    "## Configure connection to the API\n",
    "configuration = aylien_news_api.Configuration()\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '8ff61902'\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'fdb0459ccc83ea6f127c765f41f06b37'\n",
    "configuration.host = \"https://api.aylien.com/news\"\n",
    "api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))\n",
    "\n",
    "## List our parameters as search operators\n",
    "opts= {\n",
    "    'title': 'Meta, Facebook',\n",
    "    'body': 'Meta, Meta layoffs, Meta lays off, lay off, lays off, to lay off',\n",
    "    'language': ['en'],\n",
    "    'published_at_start': '2023-03-08T00:00:00Z',\n",
    "    'published_at_end': '2023-03-18T00:00:00Z',\n",
    "    'per_page': 100,\n",
    "    'sort_by': 'relevance',\n",
    "    'source_name': ['Reuters', 'Wall Street Journal', 'The Financial Times', 'Financial Times', 'CNN', 'Forbes',\n",
    "                    'The Economist', 'TechCrunch', 'Business Insider', 'Yahoo Finance', 'Bloomberg', 'CNBC']\n",
    "}\n",
    "\n",
    "try:\n",
    "    ## Make a call to the Stories endpoint for stories that meet the criteria of the search operators\n",
    "    api_response = api_instance.list_stories(**opts)\n",
    "    \n",
    "    ## Print the returned story\n",
    "    pp(api_response.stories)\n",
    "except ApiException as e:\n",
    "    print('Exception when calling DefaultApi->list_stories: %s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02784894",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following script is adapted from the Aylien “Building Your First Searches” tutorial: https://docs.aylien.com/newsapi/Build-your-first-searches/#welcome\n",
    "# The for loop is my design\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store story information\n",
    "titles = []\n",
    "bodies = []\n",
    "languages = []\n",
    "published_ats = []\n",
    "source_names = []\n",
    "story_urls = []\n",
    "\n",
    "# Loop through the stories list and extract information for each article\n",
    "for story in api_response.stories:\n",
    "    titles.append(story.title)\n",
    "    bodies.append(story.body)\n",
    "    languages.append(story.language)\n",
    "    published_ats.append(story.published_at)\n",
    "    source_names.append(story.source.name)\n",
    "    story_urls.append(story.links.permalink)\n",
    "\n",
    "# Create a pandas dataframe and convert published_ats to a datetime object\n",
    "df = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'body': bodies,\n",
    "    'language': languages,\n",
    "    'published at': published_ats,\n",
    "    'source name': source_names,\n",
    "    'story url': story_urls,\n",
    "    \n",
    "})\n",
    "df['date'] = pd.to_datetime(df['published at'])\n",
    "\n",
    "# Print the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50933033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint as pp\n",
    "\n",
    "## Configure your connection to the API\n",
    "configuration = aylien_news_api.Configuration()\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '8ff61902'\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'fdb0459ccc83ea6f127c765f41f06b37'\n",
    "configuration.host = \"https://api.aylien.com/news\"\n",
    "api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeab730",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List our parameters as search operators\n",
    "opts= {\n",
    "    'title': 'Meta, Facebook',\n",
    "    'body': 'Meta layoffs, Meta lays off, lay off, lays off',\n",
    "    'language': ['en'],\n",
    "    'published_at_start': '2023-03-08T00:00:00Z',\n",
    "    'published_at_end': '2023-03-18T00:00:00Z',\n",
    "    'per_page': 100,\n",
    "    'sort_by': 'relevance',\n",
    "    'source_name': ['Market Watch', 'The Motley Fool', 'Seeking Alpha', \n",
    "                    'Zacks', 'Zacks Investment Research','Money Morning', 'Stock Rover', 'Morningstar']\n",
    "}\n",
    "\n",
    "try:\n",
    "    ## Make a call to the Stories endpoint for stories that meet the criteria of the search operators\n",
    "    api_response = api_instance.list_stories(**opts)\n",
    "    ## Print the returned story\n",
    "    pp(api_response.stories)\n",
    "except ApiException as e:\n",
    "    print('Exception when calling DefaultApi->list_stories: %s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e037e17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following script is adapted from the Aylien “Building Your First Searches” tutorial: https://docs.aylien.com/newsapi/Build-your-first-searches/#welcome\n",
    "# The for loop is my design\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store story information\n",
    "titles = []\n",
    "bodies = []\n",
    "languages = []\n",
    "published_ats = []\n",
    "source_names = []\n",
    "story_urls = []\n",
    "\n",
    "# Loop through the stories list and extract information for each story\n",
    "for story in api_response.stories:\n",
    "    titles.append(story.title)\n",
    "    bodies.append(story.body)\n",
    "    languages.append(story.language)\n",
    "    published_ats.append(story.published_at)\n",
    "    source_names.append(story.source.name)\n",
    "    story_urls.append(story.links.permalink)\n",
    "\n",
    "# Create a pandas dataframe and add date column\n",
    "df_advisory = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'body': bodies,\n",
    "    'language': languages,\n",
    "    'published at': published_ats,\n",
    "    'source name': source_names,\n",
    "    'story url': story_urls,\n",
    "    \n",
    "})\n",
    "\n",
    "df_advisory['date'] = pd.to_datetime(df_advisory['published at'])\n",
    "\n",
    "# Print the dataframe\n",
    "df_advisory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49592778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint as pp\n",
    "\n",
    "## Configure your connection to the API\n",
    "configuration = aylien_news_api.Configuration()\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '8ff61902'\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'fdb0459ccc83ea6f127c765f41f06b37'\n",
    "configuration.host = \"https://api.aylien.com/news\"\n",
    "api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05296a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List our parameters as search operators\n",
    "opts= {\n",
    "    'title': 'Meta, Facebook',\n",
    "    'body': 'Meta, Meta layoffs, Meta lays off, lay off, lays off, to lay off',\n",
    "    'language': ['en'],\n",
    "    'published_at_start': '2022-11-03T00:00:00Z',\n",
    "    'published_at_end': '2022-11-13T00:00:00Z',\n",
    "    'per_page': 100,\n",
    "    'sort_by': 'relevance',\n",
    "    'source_name': ['Reuters', 'Wall Street Journal', 'The Financial Times', 'Financial Times', 'CNN', 'Forbes',\n",
    "                    'The Economist', 'TechCrunch', 'Business Insider', 'Yahoo Finance', 'Bloomberg', 'CNBC']\n",
    "}\n",
    "\n",
    "try:\n",
    "    ## Make a call to the Stories endpoint for stories that meet the criteria of the search operators\n",
    "    api_response = api_instance.list_stories(**opts)\n",
    "    ## Print the returned story\n",
    "    pp(api_response.stories)\n",
    "except ApiException as e:\n",
    "    print('Exception when calling DefaultApi->list_stories: %s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8d3c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following script is adapted from the Aylien “Building Your First Searches” tutorial: https://docs.aylien.com/newsapi/Build-your-first-searches/#welcome\n",
    "# The for loop is my design\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store story information\n",
    "titles = []\n",
    "bodies = []\n",
    "languages = []\n",
    "published_ats = []\n",
    "source_names = []\n",
    "story_urls = []\n",
    "\n",
    "# Loop through the stories list and extract information for each story\n",
    "for story in api_response.stories:\n",
    "    titles.append(story.title)\n",
    "    bodies.append(story.body)\n",
    "    languages.append(story.language)\n",
    "    published_ats.append(story.published_at)\n",
    "    source_names.append(story.source.name)\n",
    "    story_urls.append(story.links.permalink)\n",
    "\n",
    "# Create a pandas dataframe and convert published_ats to a datetime object\n",
    "df_nov = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'body': bodies,\n",
    "    'language': languages,\n",
    "    'published at': published_ats,\n",
    "    'source name': source_names,\n",
    "    'story url': story_urls,\n",
    "    \n",
    "})\n",
    "df_nov['date'] = pd.to_datetime(df_nov['published at'])\n",
    "\n",
    "# Print the dataframe\n",
    "df_nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import aylien_news_api\n",
    "from aylien_news_api.rest import ApiException\n",
    "from pprint import pprint as pp\n",
    "\n",
    "## Configure your connection to the API\n",
    "configuration = aylien_news_api.Configuration()\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-ID'] = '8ff61902'\n",
    "configuration.api_key['X-AYLIEN-NewsAPI-Application-Key'] = 'fdb0459ccc83ea6f127c765f41f06b37'\n",
    "configuration.host = \"https://api.aylien.com/news\"\n",
    "api_instance = aylien_news_api.DefaultApi(aylien_news_api.ApiClient(configuration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a61565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List our parameters as search operators\n",
    "opts= {\n",
    "    'title': 'Meta, Facebook',\n",
    "    'body': 'Meta layoffs, Meta lays off, lay off, lays off',\n",
    "    'language': ['en'],\n",
    "    'published_at_start': '2022-11-03T00:00:00Z',\n",
    "    'published_at_end': '2022-11-13T00:00:00Z',\n",
    "    'per_page': 100,\n",
    "    'sort_by': 'relevance',\n",
    "    'source_name': ['Market Watch', 'The Motley Fool', 'Seeking Alpha', \n",
    "                    'Zacks', 'Zacks Investment Research','Money Morning', 'Stock Rover', 'Morningstar']\n",
    "}\n",
    "\n",
    "try:\n",
    "    ## Make a call to the Stories endpoint for stories that meet the criteria of the search operators\n",
    "    api_response = api_instance.list_stories(**opts)\n",
    "    ## Print the returned story\n",
    "    pp(api_response.stories)\n",
    "except ApiException as e:\n",
    "    print('Exception when calling DefaultApi->list_stories: %s\\n' % e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbd962f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The following script is adapted from the Aylien “Building Your First Searches” tutorial: https://docs.aylien.com/newsapi/Build-your-first-searches/#welcome\n",
    "# The for loop is my design\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create empty lists to store story information\n",
    "titles = []\n",
    "bodies = []\n",
    "languages = []\n",
    "published_ats = []\n",
    "source_names = []\n",
    "story_urls = []\n",
    "\n",
    "# Loop through the stories list and extract information for each story\n",
    "for story in api_response.stories:\n",
    "    titles.append(story.title)\n",
    "    bodies.append(story.body)\n",
    "    languages.append(story.language)\n",
    "    published_ats.append(story.published_at)\n",
    "    source_names.append(story.source.name)\n",
    "    story_urls.append(story.links.permalink)\n",
    "\n",
    "# Create a pandas dataframe and add date column\n",
    "df_advisory_nov = pd.DataFrame({\n",
    "    'title': titles,\n",
    "    'body': bodies,\n",
    "    'language': languages,\n",
    "    'published at': published_ats,\n",
    "    'source name': source_names,\n",
    "    'story url': story_urls,\n",
    "    \n",
    "})\n",
    "\n",
    "df_advisory_nov['date'] = pd.to_datetime(df_advisory_nov['published at'])\n",
    "\n",
    "# Print the dataframe\n",
    "df_advisory_nov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec09a85",
   "metadata": {},
   "source": [
    "## Data Cleansing and Exploration Using Word Clouds and Topic Modeling\n",
    "Before I dive into sentiment analysis, I clean up my data using the Pandas Python library checking for duplicates and missing values. I already converted my dates into datatime format when I created the data frames. I also clean and normaliz the text data using the NLTK Python library, removing punctuation, lowercasing the letters, removing the stop words, and stemming the text.\n",
    "\n",
    "After cleaning up my data, I store the articles in text files grouped by publisher (source). I do some data exploring using words clouds and topic modeling. By looking at the words clouds and the topics being extracted, we can see that most of the articles we extracted are related to Meta’s layoffs. The code blocks below provide the script for accomplishing this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79526ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "df_advisory.drop_duplicates(inplace=True)\n",
    "df_nov.drop_duplicates(inplace=True)\n",
    "df_advisory_nov.drop_duplicates(inplace=True)\n",
    "\n",
    "# Check for missing values\n",
    "df.dropna(inplace=True)\n",
    "df_nov.dropna(inplace=True)\n",
    "df_advisory.dropna(inplace=True)\n",
    "df_advisory_nov.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ee9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"C:/Users/82106/OneDrive/바탕 화면/Text Analysis Final Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/web_scraping_workshop\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from wordcloud import WordCloud    \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35310d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/web_scraping_workshop\n",
    "# Iterate over unique source names in the dataframe\n",
    "for source_name in df['source name'].unique():\n",
    "    # Create a new dataframe for the current source\n",
    "    df_source = df[df['source name'] == source_name]\n",
    "\n",
    "    # Concatenate the 'body' column from df_nov to the current source's text\n",
    "    if source_name in df_nov['source name'].unique():\n",
    "        df_nov_source = df_nov[df_nov['source name'] == source_name]\n",
    "        source_text = ' '.join(list(df_nov_source['body']) + list(df_source['body']))\n",
    "    else:\n",
    "        source_text = ' '.join(list(df_source['body']))\n",
    "    \n",
    "    # Save the source text to a text file with the source name as the file name\n",
    "    file_name = f\"{source_name}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(source_text)\n",
    "        \n",
    "# Do the same for advisory websites: Iterate over unique source names in the dataframe\n",
    "for source_name in df_advisory['source name'].unique():\n",
    "    # Create a new dataframe for the current source\n",
    "    df_adviosory_nov_source = df_advisory[df_advisory['source name'] == source_name]\n",
    "\n",
    "    # Concatenate the 'body' column from df_nov to the current source's text\n",
    "    if source_name in df_advisory_nov['source name'].unique():\n",
    "        df_advisory_nov_source = df_advisory_nov[df_advisory_nov['source name'] == source_name]\n",
    "        source_text_advisory = ' '.join(list(df_advisory_nov_source['body']) + list(df_advisory_nov_source['body']))\n",
    "    else:\n",
    "        source_text_advisory = ' '.join(list(df_advisory_source['body']))\n",
    "    \n",
    "    # Save the source text to a text file with the source name as the file name\n",
    "    file_name = f\"{source_name}.txt\"\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(source_text_advisory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8995702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/web_scraping_workshop\n",
    "if os.path.exists('df_advisory.txt'):\n",
    "    os.remove('df_advisory.txt')\n",
    "#use glob and Path to make a list of all the filepaths in that directory and a list of all the short story titles.\n",
    "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_titles = [Path(text).stem for text in text_files]\n",
    "text_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad68d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the text files into one file to make a word cloud\n",
    "\n",
    "# Get a list of all text files in the directory\n",
    "file_list = glob.glob(directory_path + \"*.txt\")\n",
    "\n",
    "# Combine the contents of all text files into a single string\n",
    "combined_text = \"\"\n",
    "for file_path in file_list:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        file_text = f.read()\n",
    "        combined_text += file_text\n",
    "\n",
    "# Write the combined text to a new file\n",
    "with open(\"combined_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(combined_text)\n",
    "    \n",
    "if os.path.exists('combined_text.txt'):\n",
    "    os.remove('combined_text.txt')\n",
    "#use glob and Path to make a list of all the filepaths in that directory and a list of all the short story titles.\n",
    "text_files = glob.glob(f\"{directory_path}/*.txt\")\n",
    "text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad33e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two new folders to save cleaned files and another to save word cloud outputs\n",
    "#! mkdir files_cleaned\n",
    "#! mkdir wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcc6da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/web_scraping_workshop\n",
    "# Fcn source: https://medium.com/codex/a-beginners-guide-to-easily-create-a-word-cloud-in-python-7c3078c705b7\n",
    "# and https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c82ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/web_scraping_workshop\n",
    "\n",
    "#clean text\n",
    "id = 0\n",
    "lexical_density = []  \n",
    "for filepath in text_files:\n",
    "    source = os.path.splitext(os.path.basename(filepath))[0]\n",
    "    text = open(filepath, encoding='utf-8').read()\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    nltk_text = nltk.Text(text_tokens)\n",
    "    text_lower = [t.lower() for t in nltk_text if t.isalnum()]\n",
    "    text_stops = [t for t in text_lower if t not in stops]\n",
    "    text_clean = [WordNetLemmatizer().lemmatize(t, get_wordnet_pos(t)) for t in text_stops]\n",
    "    \n",
    "    # save cleaned files\n",
    "    id += 1\n",
    "    with open(f\"files_cleaned/article_cleaned_{id}.txt\", \"w\") as file:\n",
    "        file.write(str(text_clean))\n",
    "\n",
    "    # create Word Clouds\n",
    "    unique_string = (\" \").join(text_clean)\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(unique_string)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(source)\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    # save Word Clouds\n",
    "    id += 1\n",
    "    wordcloud.to_file(f\"wordclouds/word_cloud_{id}.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0a9cca",
   "metadata": {},
   "source": [
    "When we print out our word clouds, we see keywords like ‘Meta’, ‘Facebook’, ‘layoffs’, ‘employee’, ‘revenue’, ‘cut’, ‘share’, ‘stock’, and ‘Mark Zuckerberg’. Since we want to find articles related to Meta and their layoffs, this is what we want to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dc88f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/topic_modeling_workshop\n",
    "import tomotopy as tp\n",
    "import seaborn\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = stopwords.words('english')\n",
    "training_data = []\n",
    "original_texts = []\n",
    "titles = []\n",
    "\n",
    "for file in text_files:\n",
    "    text = open(file, encoding='utf-8').read()\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    nltk_text = nltk.Text(text_tokens)\n",
    "    text_lower = [t.lower() for t in nltk_text if t.isalpha()]\n",
    "    text_stops = [t for t in text_lower if t not in stops]\n",
    "    text_string = ' '.join(text_stops)\n",
    "    training_data.append(text_string)\n",
    "    original_texts.append(text)\n",
    "    titles.append(Path(file).stem)\n",
    "    \n",
    "# Number of topics to return\n",
    "num_topics = 10\n",
    "\n",
    "# Numer of topic words to print out\n",
    "num_topic_words = 10\n",
    "\n",
    "# Intialize the model\n",
    "model = tp.LDAModel(k=num_topics)\n",
    "\n",
    "# Add each document to the model, after removing white space (strip) \n",
    "# and splitting it up into words (split)\n",
    "for text in training_data:\n",
    "    model.add_doc(text.strip().split())\n",
    "    \n",
    "# The log-likelihood function is typically used to \n",
    "# derive the maximum likelihood estimator of the parameter  \n",
    "print(\"Topic Model Training...\\n\\n\")\n",
    "# Iterate over the data 10 times\n",
    "iterations = 10\n",
    "for i in range(0, 100, iterations):\n",
    "    model.train(iterations)\n",
    "    print(f'Iteration: {i}\\tLog-likelihood: {model.ll_per_word}')\n",
    "    \n",
    "\n",
    "print(\"\\nTopic Model Results:\\n\\n\")\n",
    "# Print out top 10 words for each topic\n",
    "topics = []\n",
    "topic_individual_words = []\n",
    "for topic_number in range(0, num_topics):\n",
    "    topic_words = ' '.join(word for word, prob in model.get_topic_words(topic_id=topic_number, top_n=num_topic_words))\n",
    "    topics.append(topic_words)\n",
    "    topic_individual_words.append(topic_words.split())\n",
    "    print(f\"✨Topic {topic_number}✨\\n\\n{topic_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ec1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our web scraping curriculum: https://github.com/rskrisel/topic_modeling_workshop\n",
    "topic_distributions = [list(doc.get_topic_dist()) for doc in model.docs]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', font_scale=1.2)\n",
    "def plot_categories_by_topics_heatmap(labels, \n",
    "                                      topic_distributions, \n",
    "                                      topic_keys, \n",
    "                                      output_path=None,\n",
    "                                      target_labels=None,\n",
    "                                      color_map = sns.cm.rocket_r,\n",
    "                                      dim=None):\n",
    "    \n",
    "    # Combine the labels and distributions into a list of dictionaries.\n",
    "#     The zip() function takes iterables (can be zero or more), aggregates them in a tuple, and returns it.\n",
    "    dicts_to_plot = []\n",
    "    for _label, _distribution in zip(labels, topic_distributions):\n",
    "        if not target_labels or _label in target_labels:\n",
    "            for _topic_index, _probability in enumerate(_distribution):\n",
    "                dicts_to_plot.append({'Probability': float(_probability),\n",
    "                                      'Category': _label,\n",
    "                                      'Topic': 'Topic ' + str(_topic_index).zfill(2) + ': ' + ' '.join(topic_keys[_topic_index][:5])})\n",
    "\n",
    "    # Create a dataframe, format it for the heatmap function, and normalize the columns.\n",
    "    df_to_plot = pd.DataFrame(dicts_to_plot)\n",
    "    df_wide = df_to_plot.pivot_table(index='Category', \n",
    "                                     columns='Topic', \n",
    "                                     values='Probability')\n",
    "    df_norm_col=(df_wide-df_wide.mean())/df_wide.std()\n",
    "        \n",
    "    # Show the final plot.\n",
    "    if dim:\n",
    "        plt.figure(figsize=dim)\n",
    "    sns.set(style='ticks', font_scale=1.2)\n",
    "    ax = sns.heatmap(df_norm_col, cmap=color_map)    \n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    plt.xticks(rotation=30, ha='left')\n",
    "    plt.tight_layout()\n",
    "    if output_path:\n",
    "        plt.savefig(output_path)\n",
    "    plt.show()\n",
    "    \n",
    "target_labels = titles\n",
    "plot_categories_by_topics_heatmap(titles,\n",
    "                                  topic_distributions,\n",
    "                                  topic_individual_words,\n",
    "                                  target_labels=target_labels,\n",
    "                                  color_map = 'Blues',\n",
    "                                 dim=(12,9))\n",
    "# For all possible color maps, see https://matplotlib.org/stable/tutorials/colors/colormaps.html#miscellaneous    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af9946",
   "metadata": {},
   "source": [
    "Running a topic modeling analysis gives us similar results. All topics extracted seem to reflect the topics we’re interested in. Although the topic modeling exercise signals that we’re on track and we extracted the right kind of data, it doesn’t really offer us any diversity in terms of topics. It’s just repeating the same kind of topics over and over again. \n",
    "\n",
    "Let’s move on to sentiment analysis, which will give us more insight into what online sentiment is like regarding Meta’s decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a741f1a",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "I decided to conduct separate analyses on financial news provider sentiment and financial advisory website sentiment to see if one of them offers better descriptive power with regards to stock prices. For example, sentiment trends on financial advisory websites might be more closely aligned with stock price trends across the week than that of news outlets. In that case, I would conclude that financial advisory websites like The Motley Fool or Money Watch are better at explaining (and potentially influencing) stock prices than news outlets like The Wall Street Journal.\n",
    "\n",
    "I calculate sentiment scores across each article published using VADER as we did in class, which returns a compound score between -1 (very negative) and 1 (very positive). I then add the compound sentiment score as a column in the data frames we created. \n",
    "\n",
    "I visualize the data frames using plotly express. Trend lines may be cut off when data is missing on that date. This might happen when the stock market is closed or if no articles about Meta were published on that date. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76333340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our sentiment analysis class curriculum: https://github.com/rskrisel/sentiment_analysis_workshop\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER so we can use it later\n",
    "sentimentAnalyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    scores = sentimentAnalyser.polarity_scores(text)\n",
    "    compound_score = scores['compound']\n",
    "    return compound_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e33b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis for November 2022 (first round of layoffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee50f7c",
   "metadata": {},
   "source": [
    "### Sentiment Analysis for First Round Layoffs in Nov, 2022\n",
    "Let’s first take a look at how the trends in sentiment scores and share prices compare around the time when the first round of layoffs was announced. Note that the market was closed Nov 12-13th 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c89f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding sentiment scores to the dataframe\n",
    "df_nov['sentiment score'] = df_nov['body'].apply(calculate_sentiment)\n",
    "df_nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222ed380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from Plotly Graphing Libraries “Multiple Axes in Python” tutorial: https://plotly.com/python/multiple-axes/#two-y-axes\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df_nov.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the sentiment scores to daily frequency\n",
    "daily_sentiment_nov = df_nov['sentiment score'].resample('D').mean().reset_index()\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sentiment_nov['date'], y=daily_sentiment_nov['sentiment score'], name=\"Sentiment Score\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_stock_nov['Date'], y=df_stock_nov['Close'], name=\"Stock Price\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add vertical line for layoff announcement\n",
    "fig.add_shape(\n",
    "    dict(\n",
    "        type=\"line\",\n",
    "        x0='2022-11-09',\n",
    "        y0=-1,\n",
    "        x1='2022-11-09',\n",
    "        y1=1,\n",
    "        line=dict(color='black', width=1, dash='dot'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add label for layoff announcement\n",
    "fig.add_annotation(\n",
    "    dict(\n",
    "        x='2022-11-09',\n",
    "        y=0.7,\n",
    "        xref=\"x\", yref=\"y\", text=\"Layoff Announcement\",\n",
    "        showarrow=True,\n",
    "        font=dict(size=12, color='black'),\n",
    "        align=\"center\", \n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,arrowcolor='black',\n",
    "        ax=0, ay=-30,\n",
    "        bordercolor='black', borderwidth=1,borderpad=4,\n",
    "        bgcolor='white',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Layoff Sentiment in Financial News (First Round of Layoffs)\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date (Market closed Nov 12-13)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (compound)\", secondary_y=False, range = [-1, 1])\n",
    "fig.update_yaxes(title_text=\"Stock Price ($)\", secondary_y=True, range = [80, 140])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e216c",
   "metadata": {},
   "source": [
    "Here’s a visualization for financial news sentiment vs. stock prices first. It seems that there’s been some fluctuation in sentiment during the week of the layoffs announcement. While sentiment regarding Meta was negative (-0.377) on Nov 8th,  a day before the layoffs were announced, it shot up to 0.277 on Nov 9th and dropped down the next day to stayed neutral for the next two days. During the same time frame, share prices increased dramatically. Meta shares closed at \\\\$101.47 on Nov 9th, a 5\\% increase from the previous day’s closing price of \\\\$96.47. The next day it increased a further 10\\% to \\\\$111.87. \n",
    "\n",
    "Let's continue and look at a similar visualization, this time for financial advisory websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f311ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding sentiment scores to the advisory website dataframe\n",
    "df_advisory_nov['sentiment score'] = df_advisory_nov['body'].apply(calculate_sentiment)\n",
    "df_advisory_nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afebe62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The following script is adapted from Plotly Graphing Libraries “Multiple Axes in Python” tutorial: https://plotly.com/python/multiple-axes/#two-y-axes\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df_advisory_nov.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the sentiment scores to daily frequency\n",
    "daily_sentiment_adv_nov = df_advisory_nov['sentiment score'].resample('D').mean().reset_index()\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sentiment_adv_nov['date'], y=daily_sentiment_adv_nov['sentiment score'], name=\"Sentiment Score\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_stock_nov['Date'], y=df_stock_nov['Close'], name=\"Stock Price\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add vertical line for layoff announcement\n",
    "fig.add_shape(\n",
    "    dict(\n",
    "        type=\"line\",\n",
    "        x0='2022-11-09',\n",
    "        y0=-1,\n",
    "        x1='2022-11-09',\n",
    "        y1=1,\n",
    "        line=dict(color='black', width=1, dash='dot'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add label for layoff announcement\n",
    "fig.add_annotation(\n",
    "    dict(\n",
    "        x='2022-11-09',\n",
    "        y=0.7,\n",
    "        xref=\"x\", yref=\"y\", text=\"Layoff Announcement\",\n",
    "        showarrow=True,\n",
    "        font=dict(size=12, color='black'),\n",
    "        align=\"center\", \n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,arrowcolor='black',\n",
    "        ax=0, ay=-30,\n",
    "        bordercolor='black', borderwidth=1,borderpad=4,\n",
    "        bgcolor='white',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Layoff Sentiment in Financial Advisory Pages (First Round of Layoffs)\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date (Market closed Nov 12-13)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (compound)\", secondary_y=False, range = [-1, 1])\n",
    "fig.update_yaxes(title_text=\"Stock Price ($)\", secondary_y=True, range = [80, 140])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad81cf2",
   "metadata": {},
   "source": [
    "Although there has been  fluctuation in sentiment during the week, sentiment stayed positive on large and only dropped to negative on Nov 12th. Sentiment scores dropped to 0.083 on Nov 9th when layoffs were announced but came back up to 0.509 on Nov 10th and continued to hike on Nov 11th. Contrary to news sentiment, advisory website sentiment decreased sharply when layoffs were announced but immediately rebounded to follow a similar trend with stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3097385",
   "metadata": {},
   "source": [
    "### Sentiment Analysis for Second Round Layoffs in March, 2023\n",
    "What about a second round of layoffs? Let’s look at how the trends in sentiment scores and share prices were like in March 2023, when the second round of layoffs were announced by Meta. Note that the market was closed March 11-12th 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234238c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding sentiment scores to the dataframe\n",
    "df['sentiment score'] = df['body'].apply(calculate_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from Plotly Graphing Libraries “Multiple Axes in Python” tutorial: https://plotly.com/python/multiple-axes/#two-y-axes\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the sentiment scores to daily frequency\n",
    "daily_sentiment = df['sentiment score'].resample('D').mean().reset_index()\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sentiment['date'], y=daily_sentiment['sentiment score'], name=\"Sentiment Score\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_stock_march['Date'], y=df_stock_march['Close'], name=\"Stock Price\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add vertical line for layoff announcement\n",
    "fig.add_shape(\n",
    "    dict(\n",
    "        type=\"line\",\n",
    "        x0='2023-03-14',\n",
    "        y0=-1,\n",
    "        x1='2023-03-14',\n",
    "        y1=1,\n",
    "        line=dict(color='black', width=1, dash='dot'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add label for layoff announcement\n",
    "fig.add_annotation(\n",
    "    dict(\n",
    "        x='2023-03-14',\n",
    "        y=0.8,\n",
    "        xref=\"x\", yref=\"y\", text=\"Layoff Announcement\",\n",
    "        showarrow=True,\n",
    "        font=dict(size=12, color='black'),\n",
    "        align=\"center\", \n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,arrowcolor='black',\n",
    "        ax=0, ay=-30,\n",
    "        bordercolor='black', borderwidth=1,borderpad=4,\n",
    "        bgcolor='white',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Layoff Sentiment in Financial News\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date (Market closed Mar 11-12)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (compound)\", secondary_y=False, range = [-1, 1])\n",
    "fig.update_yaxes(title_text=\"Stock Price ($)\", secondary_y=True, range = [170, 230])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bcd550",
   "metadata": {},
   "source": [
    "Here’s a visualization for financial news sentiment vs. stock prices in March. Sentiment scores dropped slightly to 0.692 on March 14th when layoffs were announced but rose again in the coming days and stayed much positive overall compared to the initial layoffs in November. During the same time frame, share prices increased dramatically. Meta shares closed at \\\\$194.02 on March 14th, a 7.25% increase from the previous day’s closing price of \\\\$180.90. The next day it increased a further 1.95% to \\\\$197.75. \n",
    "\n",
    "Let’s continue to look at advisory website sentiment vs. stock prices in the same week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6719101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding sentiment scores to the dataframe for advisory websites\n",
    "df_advisory['sentiment score'] = df_advisory['body'].apply(calculate_sentiment)\n",
    "df_advisory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafbffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from Plotly Graphing Libraries “Multiple Axes in Python” tutorial: https://plotly.com/python/multiple-axes/#two-y-axes\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "df_advisory.set_index('date', inplace=True)\n",
    "\n",
    "# Resample the sentiment scores to daily frequency\n",
    "daily_sentiment_adv = df_advisory['sentiment score'].resample('D').mean().reset_index()\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=daily_sentiment_adv['date'], y=daily_sentiment_adv['sentiment score'], name=\"Sentiment Score\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=df_stock_march['Date'], y=df_stock_march['Close'], name=\"Stock Price\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add vertical line for layoff announcement\n",
    "fig.add_shape(\n",
    "    dict(\n",
    "        type=\"line\",\n",
    "        x0='2023-03-14',\n",
    "        y0=-1,\n",
    "        x1='2023-03-14',\n",
    "        y1=1,\n",
    "        line=dict(color='black', width=1, dash='dot'),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add label for layoff announcement\n",
    "fig.add_annotation(\n",
    "    dict(\n",
    "        x='2023-03-14',\n",
    "        y=0.85,\n",
    "        xref=\"x\", yref=\"y\", text=\"Layoff Announcement\",\n",
    "        showarrow=True,\n",
    "        font=dict(size=12, color='black'),\n",
    "        align=\"center\", \n",
    "        arrowhead=2, arrowsize=1, arrowwidth=2,arrowcolor='black',\n",
    "        ax=0, ay=-30,\n",
    "        bordercolor='black', borderwidth=1,borderpad=4,\n",
    "        bgcolor='white',\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Layoff Sentiment in Financial Advisory Pages\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Date (Market closed Mar 11-12)\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"Sentiment Score (compound)\", secondary_y=False, range = [-1, 1])\n",
    "fig.update_yaxes(title_text=\"Stock Price ($)\", secondary_y=True, range = [170, 230])\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cebc71",
   "metadata": {},
   "source": [
    "Although there has been  fluctuation in sentiment during the week, sentiment stayed positive on large and only dropped to negative on March 12th. Sentiment scores dropped slightly to 0.422 on March 14th when layoffs were announced but came back up to 0.770 on March 15th and hiked down again afterwards. The sentiment scores from advisory websites seem more closely aligned with the trend line in stock prices than that of news outlets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4749bb6d",
   "metadata": {},
   "source": [
    "## Revisiting My Hypotheses\n",
    "**Hypothesis 1:** Overall, it looks like layoffs are generally met with a lower sentiment response online (though still positive) but adjust quickly to a higher score in the days following. In contrast, the stock market responds extremely positively to the announcement of layoffs. This is slightly divergent from my hypothesis of there being a linear positive trend between sentiment scores and stock prices. This could signal that online sentiment can be reactive, not predictive, of the stock market. Perhaps news outlets and financial advisory websites adjusted their sentiment after they saw that stock performance was good.\n",
    "\n",
    ">Note: When my findings were different from my first hypothesis, my initial conclusion was that sentiment analysis is not a good tool to capture a relationship with stock market reactions because I thought VADER wasn’t able to distinguish the connotation of finance specific keywords like ‘buy’ or ‘sell’. However, I noticed that it was correctly reflecting trends in the stock market in sentiment scores *after* the layoffs were announced so now I believe that my first conclusion might have been a hasty one.\n",
    "\n",
    "**Hypothesis 2:** My second hypothesis was that the first round of layoffs might alarm investors, create negative online sentiment, and drive down stock prices, but a second round of layoffs might signal business prudentce, create positive online sentiment, and lead to higher stock prices. Once again this hypothesis was only met halfway. The second round of layoffs were indeed met with a more positive online response than the first round. On average, sentiment scores are much higher in articles published in March than in November, suggesting that the second round of layoffs is being perceived more positively. Although layoffs can signal a company is struggling, they can also be a positive sign to investors that a company is making moves to remain profitable (__[Forbes, 2023](https://www.forbes.com/sites/brianbushard/2023/03/07/meta-stock-climbs-after-reports-of-more-layoffsheres-why/?sh=7527c8734f52)__). Here I suggest that the positive signaling effect is greater in the second round of layoffs, and is met with better media reception.\n",
    "\n",
    "However, my hypothesis regarding the first round of layoffs was wrong. In fact, the first round of layoffs were not perceived negatively. Sentiment scores from news media and advisory websites were both positive, just lower than that for the second round of layoffs. Furthermore, the stock market reacted very positively to both rounds of layoff announcements. This implies that layoffs’ positive signaling effect to investors was generally greater to investors than the negative signaling effect in the case of Meta. Further research is encouraged to check if this is the case for other Big Tech companies as well.\n",
    "\n",
    "**Hypothesis 3:** My last hypothesis was that I also find that on average, financial advisory websites seem to follow stock prices more closely than financial news media. However, this only holds true *after* a major event like layoffs happens. Their trend lines are on average, more aligned with the stock market than news sentiment trend lines after Nov 9th and March 14th. Further research would be needed to conclude whether one or the other type of publisher is better at actually influencing the stock market *during* (the day of) a major company event. The fact that online sentiment on advisory websites and stock prices are aligned *after* the layoff announcement might mean that there is a real time sentiment mismatch between advisory websites catered to retail investors and larger institutional investors (not reflected in my sentiment analysis) who rely on their own investment principles. Because an event like mass company layoffs are traditionally perceived as negative, financial media might initially perceive this as a negative sign and sentiment scores fall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25909b0a",
   "metadata": {},
   "source": [
    "## Conclusion: Relevance for the Financial and Policymaking Community\n",
    "\n",
    "There is a growing body of research that suggests that online sentiment can have a significant impact on share prices. Negative online sentiment of a company can lead to a decline in intraday stock prices, while positive online sentiment can lead to an increase in stock prices (__[Renault, 2017](https://www.sciencedirect.com/science/article/pii/S0378426617301589)__, and __[Wang, Yu, and Shen, 2020](https://www.hindawi.com/journals/complexity/2020/4754025/)__).\n",
    "\n",
    "My research suggests that such findings may not necessarily hold in the case of a major company event like mass layoffs, where public sentiment/perception of company value might be just one step behind that of institutional and larger investors. Larger institutional investors might be better at interpreting mixed signal events like layoffs, and drive the stock market up during the same day. To confirm my new hypothesis, it would be necessary to conduct more research on other companies’ layoff events and incorporate a larger time frame to see if there’s a similar trend.\n",
    "\n",
    "This is an important finding for investors, policymakers, and the tech industry as a whole. Although financial sentiment analysis is a trendy tool that is being increasingly welcomed by the financial community, it suggests that online sentiment may not necessarily be reliable in the face of a company event that can send mixed signals. This might be especially relevant when it comes to protecting retail investors that might rely more on financial advisory websites than bigger investors. Policymakers should develop a better understanding for when online sentiment analysis accurately reflects market performance when designing regulations regarding tech and finance industries. It should further investigate which type of investors employ these tools more and build measures to protect smaller investors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba074be2",
   "metadata": {},
   "source": [
    ">**Annex: Note on TF-IDF**\n",
    "\n",
    ">In hopes that it would give me a more concrete connection between buy and sell sentiments and stock price, I tried to perform TF-IDF analysis. Unfortunately, its results were disappointing as it didn’t seem to give much weight to buy, sell, or hold keywords. As we can see in the TF-IDF heatmap, none of the keywords that I flagged as important (in red dots) were included.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4faab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our TF-IDF curriculum: https://github.com/rskrisel/tf-idf/blob/main/README.md\n",
    "#import necessary packages for TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 600\n",
    "from pathlib import Path  \n",
    "import glob\n",
    "\n",
    "#initialize TfidfVectorizer and convert the data into a matrix format necessary to perform the tf-idf calculations\n",
    "tfidf_vectorizer = TfidfVectorizer(input='filename', stop_words='english')\n",
    "tfidf_vector = tfidf_vectorizer.fit_transform(text_files)\n",
    "type(tfidf_vector)\n",
    "\n",
    "#make a DataFrame out of the resulting tf–idf vector and add column for document frequency\n",
    "tfidf_df = pd.DataFrame(tfidf_vector.toarray(), index=text_titles, columns=tfidf_vectorizer.get_feature_names_out())\n",
    "tfidf_df.loc['00_Document Frequency'] = (tfidf_df > 0).sum()\n",
    "\n",
    "#drop column names that are numerical\n",
    "tfidf_df = tfidf_df.loc[:, ~tfidf_df.columns.str.isnumeric()]\n",
    "\n",
    "#specifying words\n",
    "tfidf_slice = tfidf_df[['layoff', 'buy', 'hold', 'sell', 'profit', 'loss', 'efficient', \n",
    "                        'revenue', 'cost', 'stock', 'hiring']]\n",
    "tfidf_slice.sort_index().round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b0cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1513d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following script is adapted from our TF-IDF curriculum: https://github.com/rskrisel/tf-idf/blob/main/README.md\n",
    "tfidf_df = tfidf_df.drop('00_Document Frequency', errors='ignore')\n",
    "tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.stack().reset_index()\n",
    "tfidf_df = tfidf_df.rename(columns={0:'tfidf', 'level_0': 'source','level_1': 'term'})\n",
    "tfidf_df.sort_values(by=['source','tfidf'], ascending=[True,False]).groupby(['source']).head(10)\n",
    "top_tfidf = tfidf_df.sort_values(by=['source','tfidf'], ascending=[True,False]).groupby(['source']).head(10)\n",
    "\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "# Terms in this list will get a red dot in the visualization\n",
    "term_list = ['buy', 'sell', 'hold', 'profit', 'loss']\n",
    "\n",
    "# adding a little randomness to break ties in term ranking\n",
    "top_tfidf_plusRand = top_tfidf.copy()\n",
    "top_tfidf_plusRand['tfidf'] = top_tfidf_plusRand['tfidf'] + np.random.rand(top_tfidf.shape[0])*0.0001\n",
    "\n",
    "# base for all visualizations, with rank calculation\n",
    "base = alt.Chart(top_tfidf_plusRand).encode(\n",
    "    x = 'rank:O',\n",
    "    y = 'source:N'\n",
    ").transform_window(\n",
    "    rank = \"rank()\",\n",
    "    sort = [alt.SortField(\"tfidf\", order=\"descending\")],\n",
    "    groupby = [\"source\"],\n",
    ")\n",
    "\n",
    "# heatmap specification\n",
    "heatmap = base.mark_rect().encode(\n",
    "    color = 'tfidf:Q'\n",
    ")\n",
    "\n",
    "# red circle over terms in above list\n",
    "circle = base.mark_circle(size=100).encode(\n",
    "    color = alt.condition(\n",
    "        alt.FieldOneOfPredicate(field='term', oneOf=term_list),\n",
    "        alt.value('red'),\n",
    "        alt.value('#FFFFFF00')        \n",
    "    )\n",
    ")\n",
    "\n",
    "# text labels, white for darker heatmap colors\n",
    "text = base.mark_text(baseline='middle').encode(\n",
    "    text = 'term:N',\n",
    "    color = alt.condition(alt.datum.tfidf >= 0.23, alt.value('white'), alt.value('black'))\n",
    ")\n",
    "\n",
    "# display the three superimposed visualizations\n",
    "(heatmap + circle + text).properties(width = 600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
